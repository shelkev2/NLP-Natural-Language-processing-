{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP all steps.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GYzsWkDSe6bE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing \n",
        "NLP is a branch of data science that consists of systematic processes for analyzing, understanding, and deriving information from the text data in a smart and efficient manner"
      ]
    },
    {
      "metadata": {
        "id": "kkaKMC6Te81X",
        "colab_type": "code",
        "outputId": "f7bf7687-38f7-49c0-f2d8-ad519e4b1bde",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "cell_type": "code",
      "source": [
        "# 1 import data set\n",
        "\n",
        "import numpy as np   \n",
        "import pandas as pd  \n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "  \n",
        "  \n",
        " \n",
        "# Import dataset \n",
        "#dataset = pd.read_csv('E:/ML/NLP/Restaurant_Reviews.tsv', delimiter = '\\t')  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-352b5e58-aef7-42d2-bf94-b10290c7bf38\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-352b5e58-aef7-42d2-bf94-b10290c7bf38\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Restaurant_Reviews.tsv to Restaurant_Reviews.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "USG9jxkvgTIy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "data = io.BytesIO(uploaded['Restaurant_Reviews.tsv'])    \n",
        "\n",
        "dataset = pd.read_csv(data,delimiter = '\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MjuQDP_gfuK-",
        "colab_type": "code",
        "outputId": "4c96588a-b81a-4ed2-d0cd-285cb0c6ca47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "dataset[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "dN6LIsfGibR9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2  Text Preprocessing/Cleaning\n",
        "\n",
        "Remove Punctuations, Numbers: Punctuations, Numbers doesn’t help much in processong the given text, if included, they will just increase the size of bag of words that we will create as last step and decrase the efficency of algorithm.   \n",
        "\n",
        "Stemming:  removal of suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
        "\n",
        "Lemmatization : obtain root word\n",
        "\n",
        "Ex:-    Fishing, fisher, fished----> fish\n",
        "\n",
        "\n",
        "\n",
        "1) Noise Removal: removal of stop words\n",
        "\n",
        "2) Lexicon Normalization \n",
        "   1)Lemmatization \n",
        "   2)Stemming\n",
        "\n",
        "\n",
        "Convert each word into its lower case\n",
        "\n",
        " \n",
        " 3) Object Standardization \n",
        "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models. \n",
        "\n",
        "Some of the examples are – acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed "
      ]
    },
    {
      "metadata": {
        "id": "DmgUF3MQiN1u",
        "colab_type": "code",
        "outputId": "71298c40-867e-49d8-9c58-64074ddb72b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# library to clean data \n",
        "import re  \n",
        "  \n",
        "# Natural Language Tool Kit \n",
        "import nltk  \n",
        "  \n",
        "nltk.download('stopwords') \n",
        "  \n",
        "# to remove stopword \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# for stemming purpose\n",
        "# for Stemming propose  \n",
        "from nltk.stem.porter import PorterStemmer \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Initialize empty array \n",
        "# to append clean text  \n",
        "cor = [] \n",
        "\n",
        "# 1000 (reviews) rows to clean \n",
        "for i in range(0, 1000):  \n",
        "      \n",
        "    # column : \"Review\", row ith \n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])  \n",
        "      \n",
        "    # convert all cases to lower cases \n",
        "    review = review.lower()  \n",
        "      \n",
        "    # split to array(default delimiter is \" \") \n",
        "    review = review.split()  \n",
        "      \n",
        "    # creating PorterStemmer object to \n",
        "    # take main stem of each word \n",
        "    ps = PorterStemmer()  \n",
        "      \n",
        "    # loop for stemming each word \n",
        "    # in string array at ith row     \n",
        "    review = [ps.stem(word) for word in review \n",
        "                if not word in set(stopwords.words('english'))]  \n",
        "                  \n",
        "    # rejoin all string array elements \n",
        "    # to create back into a string \n",
        "    review = ' '.join(review)   \n",
        "      \n",
        "    # append each string to create \n",
        "    # array of clean text  \n",
        "    cor.append(review) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TdMtRhx4j-30",
        "colab_type": "code",
        "outputId": "b961675c-4b0f-4270-dd51-f624a2f1dc17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "cor [0:10]   # after stemming & lemmatization"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wow love place',\n",
              " 'crust good',\n",
              " 'tasti textur nasti',\n",
              " 'stop late may bank holiday rick steve recommend love',\n",
              " 'select menu great price',\n",
              " 'get angri want damn pho',\n",
              " 'honeslti tast fresh',\n",
              " 'potato like rubber could tell made ahead time kept warmer',\n",
              " 'fri great',\n",
              " 'great touch']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "73887s7jlkAe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# or"
      ]
    },
    {
      "metadata": {
        "id": "IVtuWARMkq_A",
        "colab_type": "code",
        "outputId": "5f997481-b1d3-46aa-c34f-0e9aa5dee444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "stem = PorterStemmer()\n",
        "\n",
        "\n",
        "word = \"multiplying\" \n",
        "lem.lemmatize(word)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multiplying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "R4Y22z5glNX2",
        "colab_type": "code",
        "outputId": "cbb54234-0842-4e3b-b3df-d68a37c4e1e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "stem.stem(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'multipli'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "6f-TU3wWmKnD",
        "colab_type": "code",
        "outputId": "c8755063-5eb6-4bbb-b1f0-e0de396d55a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "abc = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
        "def aa(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in abc:\n",
        "            word = abc[word.lower()]\n",
        "        new_words.append(word)\n",
        "        new_text = \" \".join(new_words) \n",
        "    return new_text\n",
        "\n",
        "aa(\"RT this is a retweeted dm tweet by Shivam Bansal\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Retweet this is a retweeted direct message tweet by Shivam Bansal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "TvnEBIFYmx0N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEf2BMN0of60",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3   Tokenization, involves splitting sentences and words from the body of the text\n",
        "\n",
        "Text to Features (Feature Engineering on text data)  \n",
        "To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques –\n",
        " 1) Syntactical Parsing,  \n",
        " 2) Entities / N-grams / word-based features,  \n",
        " 3) Statistical features,  and  \n",
        " 4)word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "4GXqQuqEpJrv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kyX_kChjpLKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1  Syntactical parsing\n",
        "involves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words\n",
        "\n",
        "\n",
        "Part of speech tagging –   \n",
        "Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence\n"
      ]
    },
    {
      "metadata": {
        "id": "F_bjGecnpZIv",
        "colab_type": "code",
        "outputId": "883c91e3-dcb5-4aa2-8004-61738927053a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk import word_tokenize, pos_tag\n",
        "text = \"I am learning Natural Language Processing using google\"\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print( pos_tag(tokens) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('using', 'VBG'), ('google', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BplgYhLyqUdR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) 2 Entity Extraction (Entities as features)\n",
        "\n",
        "Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.\n",
        "\n",
        "Topic Modelling & Named Entity Recognition are the two key entity detection methods in NLP.\n",
        "\n",
        "# A. Named Entity Recognition (NER)\n",
        "\n",
        "The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :\n",
        "\n",
        "Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.\n",
        "\n",
        "Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)\n",
        "\n",
        "A typical NER model consists of three blocks:\n",
        "\n",
        "1Noun phrase identification: \n",
        "This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.\n",
        "\n",
        "\n",
        " 2Phrase classification: \n",
        "This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.\n",
        "\n",
        "\n",
        "3Entity disambiguation: \n",
        "Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia. \n",
        "\n",
        "\n",
        "# B.  Topic Modeling\n",
        "Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.  \n",
        "\n",
        "\n",
        "Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique, Following is the code to implement topic modeling using LDA in python."
      ]
    },
    {
      "metadata": {
        "id": "eZ5IMOGxpeHe",
        "colab_type": "code",
        "outputId": "ca79b07f-a48f-41bc-9a6c-7e2836ddb323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "\n",
        "\n",
        "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
        "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_clean = [doc.split() for doc in doc_complete]\n",
        "\n",
        "import gensim from gensim\n",
        "import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
        "\n",
        "# Results \n",
        "print(ldamodel.print_topics())\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\ndoc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \\ndoc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\\ndoc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\\ndoc_complete = [doc1, doc2, doc3]\\ndoc_clean = [doc.split() for doc in doc_complete]\\n\\nimport gensim from gensim\\nimport corpora\\n\\n# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \\ndictionary = corpora.Dictionary(doc_clean)\\n\\n# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \\ndoc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\\n\\n# Creating the object for LDA model using gensim library\\nLda = gensim.models.ldamodel.LdaModel\\n\\n# Running and Training LDA model on the document term matrix\\nldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\\n\\n# Results \\nprint(ldamodel.print_topics())\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "S2_BovjFviPT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# C.  N-Grams as Features  \n",
        "A combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text."
      ]
    },
    {
      "metadata": {
        "id": "A31AajFMs1VY",
        "colab_type": "code",
        "outputId": "0665baa6-c5ac-49d9-b571-4e8d845d99c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    output = []  \n",
        "    for i in range(len(words)-n+1):\n",
        "        output.append(words[i:i+n])\n",
        "    return output\n",
        "  \n",
        "  \n",
        "generate_ngrams('this is a sample text', 2)  \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "iph3DscYvriB",
        "colab_type": "code",
        "outputId": "dddb30c7-69e3-4ed7-8c46-b755d9b269ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "generate_ngrams('this is a sample text', 3)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'is', 'a'], ['is', 'a', 'sample'], ['a', 'sample', 'text']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "2lQa8QMFwTQQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " # 3 Statistical Features \n",
        "Text data can also be quantified directly into numbers using several techniques described in this section: \n",
        "\n",
        "A.  Term Frequency – Inverse Document Frequency (TF – IDF)   \n",
        "TF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example – let say there is a dataset of N text documents, In any document “D”, TF and IDF will be defined as –\n",
        "\n",
        "\n",
        "#Term Frequency (TF) – \n",
        "TF for a term “t” is defined as the count of a term “t” in a document “D”\n",
        "\n",
        "#Inverse Document Frequency (IDF) – \n",
        "IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T."
      ]
    },
    {
      "metadata": {
        "id": "ufMqWZc5vzJ5",
        "colab_type": "code",
        "outputId": "50b3c88c-952e-4104-f36b-4c32951cecca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "obj = TfidfVectorizer()\n",
        "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
        "X = obj.fit_transform(corpus)\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 11 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "0cBAr_ymxBjj",
        "colab_type": "code",
        "outputId": "97b96238-5005-4c81-8687-fe0f18ed1213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 7)\t0.5844829010200651\n",
            "  (0, 2)\t0.5844829010200651\n",
            "  (0, 4)\t0.444514311537431\n",
            "  (0, 1)\t0.34520501686496574\n",
            "  (1, 1)\t0.3853716274664007\n",
            "  (1, 0)\t0.652490884512534\n",
            "  (1, 3)\t0.652490884512534\n",
            "  (2, 4)\t0.444514311537431\n",
            "  (2, 1)\t0.34520501686496574\n",
            "  (2, 6)\t0.5844829010200651\n",
            "  (2, 5)\t0.5844829010200651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b20J4dWyzarA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Making the bag of words via sparse matrix   \n",
        "\n",
        "Take all the different words of reviews in the dataset without repeating of words.\n",
        "One column for each word, therefore there are going to be many columns.\n",
        "Rows are reviews\n",
        "If word is there in row of dataset of reviews, then the count of word will be there in row of bag of words under the column of the word.  \n",
        "Examples: Let’s take a dataset of reviews of only two reviews  \n",
        "\n",
        "Input : \"dam good steak\", \"good food good servic\"\n",
        "\n",
        "Output :  good food servic dam  steak      \n",
        "                   1                              1        1\n",
        "                  2         1        1         \n",
        "                  \n",
        "                  \n",
        "                  \n",
        "                  \n",
        "For this purpose we need CountVectorizer class from sklearn.feature_extraction.text.\n",
        "We can also set max number of features (max no. features which help the most via attribute “max_features”). Do the training on corpus and then apply the same transformation to the corpus “.fit_transform(corpus)” and then convert it into array. If review is positive or negative that answer is in second column of : dataset[:, 1] : all rows ans 1st column (indexing from zero).                  \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "KmLIauHkxOgZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating the Bag of Words model \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "  \n",
        "# To extract max 1500 feature. \n",
        "# \"max_features\" is attribute to \n",
        "# experiment with to get better results \n",
        "cv = CountVectorizer(max_features = 1500)  \n",
        "  \n",
        "# X contains corpus (dependent variable) \n",
        "X = cv.fit_transform(cor).toarray()  \n",
        "  \n",
        "# y contains answers if review \n",
        "# is positive or negative \n",
        "y = dataset.iloc[:, 1].values  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HqRPFHd8z7N4",
        "colab_type": "code",
        "outputId": "d8fafd13-6c11-46c5-a6f2-b4650530c162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZBr7K_kz8aQ",
        "colab_type": "code",
        "outputId": "aa8aafb4-d335-4e70-ee1c-ddfbed9d1734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y[0:19]     # 0 is for negative review and 1 is for positive review"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "8xmDYHJy0Tm2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Splitting Corpus into Training and Test set. \n",
        "For this we need class train_test_split from sklearn.cross_validation. Split can be made 70/30 or 80/20 or 85/15 or 75/25, here I choose 75/25 via “test_size”.\n",
        "\n",
        "X is the bag of words, y is 0 or 1 (positive or negative)"
      ]
    },
    {
      "metadata": {
        "id": "kkFGCTzoz927",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "  \n",
        "# experiment with \"test_size\" \n",
        "# to get better results \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AUt85gA7cPVG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Fitting a Predictive Model (here random forest)"
      ]
    },
    {
      "metadata": {
        "id": "dzsjxfrW0eKn",
        "colab_type": "code",
        "outputId": "7ee43643-b236-4a7c-c74e-4dab54600149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# to the Training set \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "  \n",
        "# n_estimators can be said as number of \n",
        "# trees, experiment with n_estimators \n",
        "# to get better results  \n",
        "model = RandomForestClassifier(n_estimators = 501, \n",
        "                            criterion = 'entropy') \n",
        "                              \n",
        "model.fit(X_train, y_train)  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
              "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "            min_samples_leaf=1, min_samples_split=2,\n",
              "            min_weight_fraction_leaf=0.0, n_estimators=501, n_jobs=None,\n",
              "            oob_score=False, random_state=None, verbose=0,\n",
              "            warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "B1VjDtGgce3K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Pridicting Final Results via using .predict() method"
      ]
    },
    {
      "metadata": {
        "id": "_tlNWVU12I7z",
        "colab_type": "code",
        "outputId": "90230f82-03fc-419a-a8ba-ea9727321f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# Predicting the Test set results \n",
        "y_pred = model.predict(X_test) \n",
        "  \n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
              "       1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
              "       0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "DI-fid3_c7dK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# To know the accuracy, confusion matrix is needed."
      ]
    },
    {
      "metadata": {
        "id": "1YpMsjXAcnWD",
        "colab_type": "code",
        "outputId": "76efa058-fdaf-48e6-98e0-4e8a83c19a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Making the Confusion Matrix \n",
        "from sklearn.metrics import confusion_matrix \n",
        "  \n",
        "cm = confusion_matrix(y_test, y_pred) \n",
        "  \n",
        "cm "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[110,  11],\n",
              "       [ 56,  73]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "mQLhOK4AeiWB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Uses/ tasks of NLP\n",
        "# 1) text classification\n",
        "\n",
        "Text classification is one of the classical problem of NLP. Notorious examples include – Email Spam Identification, topic classification of news, sentiment classification and organization of web pages by search engines"
      ]
    },
    {
      "metadata": {
        "id": "15YisN4veQaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
        "from textblob import TextBlob\n",
        "training_corpus = [\n",
        "                   ('I am exhausted of this work.', 'Class_B'),\n",
        "                   (\"I can't cooperate with this\", 'Class_B'),\n",
        "                   ('He is my badest enemy!', 'Class_B'),\n",
        "                   ('My management is poor.', 'Class_B'),\n",
        "                   ('I love this burger.', 'Class_A'),\n",
        "                   ('This is an brilliant place!', 'Class_A'),\n",
        "                   ('I feel very good about these dates.', 'Class_A'),\n",
        "                   ('This is my best work.', 'Class_A'),\n",
        "                   (\"What an awesome view\", 'Class_A'),\n",
        "                   ('I do not like this dish', 'Class_B')]\n",
        "test_corpus = [\n",
        "                (\"I am not feeling well today.\", 'Class_B'), \n",
        "                (\"I feel brilliant!\", 'Class_A'), \n",
        "                ('Gary is a friend of mine.', 'Class_A'), \n",
        "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
        "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eCMTevtge6mj",
        "colab_type": "code",
        "outputId": "7f2a9b4d-1e12-4e8f-b88b-cd19677590bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model = NBC(training_corpus) \n",
        "print(model.classify(\"Their codes are amazing.\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OYuEGg6rfCHi",
        "colab_type": "code",
        "outputId": "7b80f6ba-09e7-4992-a220-ad8d6971cb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.classify(\"I don't like their computer.\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnX6C49rfOWU",
        "colab_type": "code",
        "outputId": "dad184c1-2792-41f6-a934-8114397ee309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(model.accuracy(test_corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HIzt2Rc2fdNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# using scikit learn"
      ]
    },
    {
      "metadata": {
        "id": "C_sOYqmGfQxg",
        "colab_type": "code",
        "outputId": "3d8b422f-c6db-41a1-f2a3-8dc997c31957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import svm \n",
        "\n",
        "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
        "train_data = []\n",
        "train_labels = []\n",
        "for row in training_corpus:\n",
        "    train_data.append(row[0])\n",
        "    train_labels.append(row[1])\n",
        "\n",
        "test_data = [] \n",
        "test_labels = [] \n",
        "for row in test_corpus:\n",
        "    test_data.append(row[0]) \n",
        "    test_labels.append(row[1])\n",
        "\n",
        "# Create feature vectors \n",
        "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
        "# Train the feature vectors\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "# Apply model on test data \n",
        "test_vectors = vectorizer.transform(test_data)\n",
        "\n",
        "# Perform classification with SVM, kernel=linear \n",
        "model = svm.SVC(kernel='linear') \n",
        "model.fit(train_vectors, train_labels) \n",
        "prediction = model.predict(test_vectors)\n",
        "print(prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Class_A' 'Class_A' 'Class_B' 'Class_B' 'Class_A' 'Class_A']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P-ICPk75fkFe",
        "colab_type": "code",
        "outputId": "59b30632-a020-4cc1-9956-4166a1ec3dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "print (classification_report(test_labels, prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class_A       0.50      0.67      0.57         3\n",
            "     Class_B       0.50      0.33      0.40         3\n",
            "\n",
            "   micro avg       0.50      0.50      0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gvwoHgeGgR77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Text Matching / Similarity\n",
        "\n",
        " Important applications of text matching includes   \n",
        " automatic spelling correction,     \n",
        " data de-duplication and    \n",
        " genome analysis etc.   "
      ]
    },
    {
      "metadata": {
        "id": "1TrxBehPgCS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}